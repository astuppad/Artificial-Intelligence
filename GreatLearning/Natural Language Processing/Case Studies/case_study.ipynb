{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"colab":{"name":"case_study.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uUpQff5qfTNc"},"source":["In this case study we will be implementing an elementary model that utilizes word embeddings for text classification. Word embeddings are known for encoding contextual information. In this notebook we will use a pretrained model to generate word embeddings of each word in a sentence. Further, average of all embeddings for a sentence will be the sentence representation. Each sentence representation will be classified into one of the categories. The entire process is described step by step below:\n","\n","1. Load the dataset from the disk\n","2. Tokenize text in the dataset and create vocabulary\n","3. Load the word2vec model from the disk into a python dictionary\n","4. Load embeddings for each word and take average\n","5. One hot encode the target labels\n","6. Train the classifier"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":83368,"status":"ok","timestamp":1566386987157,"user":{"displayName":"dikshant gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBY-VAEDCe_t9LzKk0g7MBc8rY1qZ-QR-XiCIyRYw=s64","userId":"01845807612441668603"},"user_tz":-330},"id":"uyKzNXim8jEY","outputId":"eeb57946-6d78-410a-b384-1bb368a70658","colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zhXsYJwq7-Rs","colab":{}},"source":["from nltk.tokenize import RegexpTokenizer\n","import numpy as np\n","import re"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KNmyqGakl-iU"},"source":["### Load the dataset from the disk"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":7321,"status":"ok","timestamp":1566387081318,"user":{"displayName":"dikshant gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBY-VAEDCe_t9LzKk0g7MBc8rY1qZ-QR-XiCIyRYw=s64","userId":"01845807612441668603"},"user_tz":-330},"id":"s_Bu4lfx7-Rz","outputId":"97bd61bd-4a05-4365-f935-4127bf06790f","colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["import pandas as pd\n","df = pd.read_csv('complaints.csv', nrows=1000) # We are taking less rows for faster execution\n","df.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Consumer complaint narrative</th>\n","      <th>Product</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>I have outdated information on my credit repor...</td>\n","      <td>Credit reporting</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I purchased a new car on XXXX XXXX. The car de...</td>\n","      <td>Consumer Loan</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>An account on my credit report has a mistaken ...</td>\n","      <td>Credit reporting</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>This company refuses to provide me verificatio...</td>\n","      <td>Debt collection</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>This complaint is in regards to Square Two Fin...</td>\n","      <td>Debt collection</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                        Consumer complaint narrative           Product\n","0  I have outdated information on my credit repor...  Credit reporting\n","1  I purchased a new car on XXXX XXXX. The car de...     Consumer Loan\n","2  An account on my credit report has a mistaken ...  Credit reporting\n","3  This company refuses to provide me verificatio...   Debt collection\n","4  This complaint is in regards to Square Two Fin...   Debt collection"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lZRFTXec7-R7"},"source":["### Tokenizer\n","Regular expression based tokenizers to consider only alphabetical sequences and ignore numerical sequences."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"v6H1UTDM7-R8","colab":{}},"source":["def complaint_to_words(comp):\n","    \n","    words = RegexpTokenizer('\\w+').tokenize(comp)\n","    words = [re.sub(r'([xx]+)|([XX]+)|(\\d+)', '', w).lower() for w in words]\n","    words = list(filter(lambda a: a != '', words))\n","    \n","    return words"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yafpgOxe7-SA"},"source":["### Vocabulary\n","Extracing all the unique words from the dataset"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0RgXmo-N7-SC","colab":{}},"source":["all_words = list()\n","for comp in df['Consumer complaint narrative']:\n","    for w in complaint_to_words(comp):\n","        all_words.append(w)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":80284,"status":"ok","timestamp":1566387158514,"user":{"displayName":"dikshant gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBY-VAEDCe_t9LzKk0g7MBc8rY1qZ-QR-XiCIyRYw=s64","userId":"01845807612441668603"},"user_tz":-330},"id":"8T_RNzwy7-SF","outputId":"cd6c76a2-42d6-43aa-877f-f399f9799130","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print('Size of vocabulary: {}'.format(len(set(all_words))))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Size of vocabulary: 6802\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":79440,"status":"ok","timestamp":1566387158515,"user":{"displayName":"dikshant gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBY-VAEDCe_t9LzKk0g7MBc8rY1qZ-QR-XiCIyRYw=s64","userId":"01845807612441668603"},"user_tz":-330},"id":"Dbh2Y10y7-SL","outputId":"da08bf30-04f9-4ec1-c798-0f22a4c44cb9","colab":{"base_uri":"https://localhost:8080/","height":190}},"source":["print('Complaint\\n', df['Consumer complaint narrative'][10], '\\n')\n","print('Tokens\\n', complaint_to_words(df['Consumer complaint narrative'][10]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Complaint\n"," Without provocation, I received notice that my credit line was being decreased by nearly 100 %. My available credit was reduced from $ XXXX to XXXX ( the rough amount of my available balance ). \n","\n","When I called to question the change, I was provided a nob-descript response referencing my XXXX report. It was my understanding that under the FCRA I was entitled to a copy of this report, but was refused by Citi and have been given no further explanation. \n","\n","This is predatory in that it affects my utilization of credit, further subjecting me to increase in APrs, etc and a higher cost of credit without any reason. \n","\n","Tokens\n"," ['without', 'provocation', 'i', 'received', 'notice', 'that', 'my', 'credit', 'line', 'was', 'being', 'decreased', 'by', 'nearly', 'my', 'available', 'credit', 'was', 'reduced', 'from', 'to', 'the', 'rough', 'amount', 'of', 'my', 'available', 'balance', 'when', 'i', 'called', 'to', 'question', 'the', 'change', 'i', 'was', 'provided', 'a', 'nob', 'descript', 'response', 'referencing', 'my', 'report', 'it', 'was', 'my', 'understanding', 'that', 'under', 'the', 'fcra', 'i', 'was', 'entitled', 'to', 'a', 'copy', 'of', 'this', 'report', 'but', 'was', 'refused', 'by', 'citi', 'and', 'have', 'been', 'given', 'no', 'further', 'eplanation', 'this', 'is', 'predatory', 'in', 'that', 'it', 'affects', 'my', 'utilization', 'of', 'credit', 'further', 'subjecting', 'me', 'to', 'increase', 'in', 'aprs', 'etc', 'and', 'a', 'higher', 'cost', 'of', 'credit', 'without', 'any', 'reason']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YHi4vCGX7-SU"},"source":["### Indexing\n","Indexing each unique word in the dataset by assigning it a unique number."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-ClRX_y07-SW","colab":{}},"source":["index_dict = dict()\n","count = 1\n","index_dict['<unk>'] = 0\n","for word in set(all_words):\n","    index_dict[word] = count\n","    count += 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vv8dIbF47-Sa"},"source":["### Dataset\n","Utilizing indexed words to replace words by index. This makes the dataset numerical and keras readable."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"u1OJzln_7-Sb","colab":{}},"source":["embeddings_index = {}\n","f = open('glove.6B.300d.txt')\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"s--9I9d5msCp"},"source":["#### Taking average of all word embeddings in a sentence to generate the sentence representation."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aN13GyDe7-Sd","colab":{}},"source":["data_list = list()\n","for comp in df['Consumer complaint narrative']:\n","    sentence = np.zeros(300)\n","    count = 0\n","    for w in complaint_to_words(comp):\n","        try:\n","            sentence += embeddings_index[w]\n","            count += 1\n","        except KeyError:\n","            continue\n","    data_list.append(sentence / count)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OYZ703hVm5Cg"},"source":["#### Converting categrical labels to numerical format and further one hot encoding on the numerical labels."]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":1796,"status":"ok","timestamp":1566387500223,"user":{"displayName":"dikshant gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBY-VAEDCe_t9LzKk0g7MBc8rY1qZ-QR-XiCIyRYw=s64","userId":"01845807612441668603"},"user_tz":-330},"id":"EoR79rtZ7-Sr","outputId":"704c6e1e-dc8e-4d42-851d-0e3724e60fd6","colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["from sklearn import preprocessing\n","le = preprocessing.LabelEncoder()\n","le.fit(df['Product'])\n","df['Target'] = le.transform(df['Product'])\n","df.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Consumer complaint narrative</th>\n","      <th>Product</th>\n","      <th>Target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>I have outdated information on my credit repor...</td>\n","      <td>Credit reporting</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>I purchased a new car on XXXX XXXX. The car de...</td>\n","      <td>Consumer Loan</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>An account on my credit report has a mistaken ...</td>\n","      <td>Credit reporting</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>This company refuses to provide me verificatio...</td>\n","      <td>Debt collection</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>This complaint is in regards to Square Two Fin...</td>\n","      <td>Debt collection</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                        Consumer complaint narrative           Product  Target\n","0  I have outdated information on my credit repor...  Credit reporting       3\n","1  I purchased a new car on XXXX XXXX. The car de...     Consumer Loan       1\n","2  An account on my credit report has a mistaken ...  Credit reporting       3\n","3  This company refuses to provide me verificatio...   Debt collection       4\n","4  This complaint is in regards to Square Two Fin...   Debt collection       4"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"atXHKYN27-S0"},"source":["### One hot Encoding"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_RwGbO_L7-S4","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(np.array(data_list), df.Target.values, \n","    test_size=0.15, random_state=0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":1623,"status":"ok","timestamp":1566387605397,"user":{"displayName":"dikshant gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBY-VAEDCe_t9LzKk0g7MBc8rY1qZ-QR-XiCIyRYw=s64","userId":"01845807612441668603"},"user_tz":-330},"id":"K7XHJpLc7-S7","outputId":"2e39d372-d636-467b-cadc-4da71065e2a6","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(X_train.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(850, 300)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":1388,"status":"ok","timestamp":1566387619059,"user":{"displayName":"dikshant gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBY-VAEDCe_t9LzKk0g7MBc8rY1qZ-QR-XiCIyRYw=s64","userId":"01845807612441668603"},"user_tz":-330},"id":"ob2Az_Oq-h0x","outputId":"60606497-6f8c-4205-97bb-2deb2c55dbdd","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(y_train.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(850,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sw7pp-WinSI5"},"source":["#### Training and testing the classifier"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"elapsed":3057,"status":"ok","timestamp":1566387636476,"user":{"displayName":"dikshant gupta","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBY-VAEDCe_t9LzKk0g7MBc8rY1qZ-QR-XiCIyRYw=s64","userId":"01845807612441668603"},"user_tz":-330},"id":"--kThiN07-S_","outputId":"cc3e1cc6-3c65-4836-b096-4271a7dda6b5","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from sklearn.naive_bayes import BernoulliNB\n","from sklearn.metrics import accuracy_score\n","clf = BernoulliNB()\n","clf.fit(X_train, y_train)\n","pred = clf.predict(X_test)\n","print(accuracy_score(y_test, pred))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.48\n"],"name":"stdout"}]}]}